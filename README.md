# ğŸ¥ YouTube RAG Chatbot

Ask intelligent questions about any YouTube video using AI!  
This project extracts video transcripts, finds the most relevant parts using semantic search (FAISS), and generates answers using a Large Language Model (LLM).
---
## ğŸ§  Why This Project?

YouTube videos are packed with knowledge â€” but finding answers from long videos is tedious.  
This project turns any YouTube video into an intelligent Q&A chatbot using Retrieval-Augmented Generation (RAG).  
Instead of watching a 20-minute video, just **ask a question** and get a smart, context-aware answer.
---
## ğŸš€ How It Works

1. ğŸ¯ **Input a YouTube URL**
2. ğŸ“ **Extract the transcript** (typed or auto-generated)
3. ğŸ“š **Chunk the transcript** for semantic search
4. ğŸ§  **Embed using SentenceTransformer**
5. ğŸ” **Store & search with FAISS**
6. ğŸ’¬ **Use an LLM (Mistral-7B)** to answer based on retrieved context
---
## ğŸ› ï¸ Tech Stack / Tools Used

| Category | Tool |
|---------|------|
| ğŸ’¬ LLM | [Mistral-7B-Instruct](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3) via Hugging Face Inference API |
| ğŸ” Embeddings | `sentence-transformers/all-MiniLM-L6-v2` |
| ğŸ“„ Transcript | `youtube-transcript-api` |
| ğŸ§  Vector DB | `FAISS` (in-memory) |
| ğŸ”— Framework | `LangChain` |
| ğŸŒ UI | `Streamlit` |
| ğŸ Language | Python 3.10+ |

---

## ğŸ–¥ï¸ Demo (Optional)

> Paste a YouTube URL and ask:  
> *â€œWhat is the main idea of this video?â€*  
> *â€œWhat tools were discussed?â€*

---

## ğŸ“¦ Installation

```bash
git clone https://github.com/yourusername/youtube-rag-chatbot.git
cd youtube-rag-chatbot

pip install -r requirements.txt

# Run the app
streamlit run app.py

ğŸ”® Possible Enhancements
 ğŸ§  Add support for multi-turn conversation

 ğŸŒ Enable multilingual transcript-based Q&A

 ğŸ’¾ Save and load FAISS index to/from disk

 ğŸ“¥ Let users upload audio/video files directly

 ğŸ§  Add OpenAI model support (e.g. gpt-3.5, gpt-4) for more accurate, faster results

 â˜ï¸ Deploy on Streamlit Cloud or Hugging Face Spaces

âš¡ Performance Tips
-If the app feels slow or times out during generation, try the following:
-âš™ï¸ Use a smaller or faster model such as:
-google/flan-t5-base
-tiiuae/falcon-7b-instruct
-HuggingFaceH4/zephyr-7b-alpha

ğŸ§¹ Reduce the number of context chunks:

Limit FAISS to top 1â€“2 matches

Truncate each chunk to 200â€“300 words

ğŸ“¶ Ensure stable internet â€” latency depends on API speed

ğŸ” Use st.session_state to avoid recomputing vector stores or embeddings

ğŸš€ Use OpenAI API key (e.g. gpt-3.5-turbo) for highly efficient, low-latency answers:
